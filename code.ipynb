import torch
import nltk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import pandas as pd

df = pd.read_csv('/content/AI-Hackathon-test-data-set.csv')
print(df.shape)

import torch
import nltk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

def download_nltk_resources():
    resources = ['punkt', 'stopwords', 'wordnet']
    for resource in resources:
        try:
            nltk.download(resource, quiet=True)
        except Exception as e:
            print(f"Warning: Failed to download {resource}: {str(e)}")

class TextPreprocessor:
    def _init_(self):
        # Download required NLTK resources
        download_nltk_resources()

        try:
            self.lemmatizer = WordNetLemmatizer()
            self.stop_words = set(stopwords.words('english'))
        except Exception as e:
            print(f"Warning: Full text preprocessing unavailable: {str(e)}")
            self.lemmatizer = None
            self.stop_words = set()

    def basic_preprocess(self, text):
        # Basic preprocessing without NLTK
        text = str(text).lower()
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        return ' '.join(text.split())

    def preprocess(self, text):
        # First apply basic preprocessing
        text = self.basic_preprocess(text)

        if self.lemmatizer is None:
            return text

        try:
            # Advanced preprocessing with NLTK
            tokens = word_tokenize(text)
            tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]
            return ' '.join(tokens)
        except Exception as e:
            print(f"Warning: Falling back to basic preprocessing: {str(e)}")
            return text

def predict_sentiment(text):
    # Initialize model and tokenizer
    MODEL = "cardiffnlp/twitter-roberta-base-sentiment"
    tokenizer = AutoTokenizer.from_pretrained(MODEL)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL)

    # Initialize preprocessor
    preprocessor = TextPreprocessor()

    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()

    # Preprocess the text
    processed_text = preprocessor.preprocess(text)

    # Tokenize
    inputs = tokenizer(
        processed_text,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

    # Move inputs to device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Get prediction
    with torch.no_grad():
        outputs = model(**inputs)
        prediction = torch.argmax(outputs.logits, dim=1)
        probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)

    # Map prediction to sentiment
    sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}
    predicted_sentiment = sentiment_map[prediction.item()]
    confidence = probabilities.max().item() * 100

    return predicted_sentiment, confidence

if _name_ == "_main_":
    print("Initializing sentiment analyzer...")

    while True:
        # Get user input
        user_comment = input("\nEnter your comment for sentiment analysis (or 'quit' to exit): ")

        if user_comment.lower() == 'quit':
            break

        # Get prediction and confidence
        sentiment, confidence = predict_sentiment(user_comment)

        # Print result
        print(f"\nText: {user_comment}")
        print(f"Predicted sentiment: {sentiment}")
        print(f"Confidence: {confidence:.2f}%")
